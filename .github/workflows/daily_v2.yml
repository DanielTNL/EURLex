name: Pipeline v2 (daily)

on:
  schedule:
    # GitHub Actions cron is UTC. 06:00 UTC = 08:00 Europe/Amsterdam (CEST).
    - cron: "0 6 * * *"
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: daily-v2
  cancel-in-progress: false

env:
  TZ: Europe/Amsterdam

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      # Optional: set globally for the job if you have a revalidate endpoint
      REVALIDATE_URL: ${{ secrets.REVALIDATE_URL }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps (best-effort)
        run: |
          python -V
          pip install -r requirements.txt || true
          pip install requests python-dateutil beautifulsoup4 lxml pyyaml orjson jsonschema jq || true

      # 1) Discover laatste 24 uur (alle bronnen)
      # Guard against a missing sources_v2.yaml: if it exists, pass it; otherwise run with just config_v2.yaml
      - name: Daily discover (last 24h)
        run: |
          if [ -f sources_v2.yaml ]; then
            echo "Using sources_v2.yaml"
            python workers/weekly_discover.py --window 1d --sources sources_v2.yaml --config config_v2.yaml
          else
            echo "sources_v2.yaml not found — running with config_v2.yaml only"
            python workers/weekly_discover.py --window 1d --config config_v2.yaml
          fi

      # 2) Verwerk documenten (samenvattingen e.d.)
      - name: Process documents
        run: |
          python workers/process_document.py --from state/latest_discovery.json --config config_v2.yaml --limit 50

      # 2b) (Optional) Validate state/timeline against schema if present
      - name: Validate state/timeline schema (best-effort)
        continue-on-error: true
        run: |
          if [ -f schemas/timeline.v1.json ]; then
            python - <<'PY'
import json, glob, jsonschema, sys
from pathlib import Path
schema = json.load(open('schemas/timeline.v1.json'))
errs = 0
for f in glob.glob('state/*.json'):
    try:
        data = json.load(open(f))
        # If your files have top-level arrays or objects with "items", adapt here:
        jsonschema.validate(data, schema)
    except Exception as e:
        print(f"[schema] {f}: {e}")
        errs += 1
print(f"[schema] done with {errs} error(s)")
PY
          else
            echo "No schemas/timeline.v1.json found; skipping validation"
          fi

      # 3) Bouw rolling timeline (7 dagen)
      - name: Build timeline (rolling 7d)
        run: |
          python workers/build_timeline.py --window 7d --config config_v2.yaml

      # 4) Daily digest (MD + JSON → voor nieuwsbrief/website)
      - name: Build daily digest
        run: |
          python workers/build_daily_digest.py --hours 24

      # 5) Build site payload (prefer your project builder; fallback to v2 worker)
      - name: Build site payload
        run: |
          if [ -f build_site_data.py ]; then
            python build_site_data.py || true
          else
            python workers/build_site_data_v2.py
          fi

      # 6) Publiceer bridge: v2 → legacy paden die de front-end leest
      - name: Publish site bridge (v2 → legacy)
        run: |
          python workers/publish_site_bridge.py

      # 7) Summaries for quick inspection
      - name: Summarise counts
        continue-on-error: true
        run: |
          echo "== SUMMARY ==" >> $GITHUB_STEP_SUMMARY
          if [ -f docs/site/site_data.json ]; then
            echo "- items in docs/site/site_data.json: $(jq '.items | length' docs/site/site_data.json)" >> $GITHUB_STEP_SUMMARY || true
          fi
          if [ -f outputs/timelines/timeline_7d.json ]; then
            echo "- items in outputs/timelines/timeline_7d.json: $(jq '.items | length' outputs/timelines/timeline_7d.json)" >> $GITHUB_STEP_SUMMARY || true
          fi
          if [ -f reports/daily/latest.md ]; then
            echo "- daily report generated ✅" >> $GITHUB_STEP_SUMMARY
          fi

      # 8) Inspectie in logs
      - name: Show outputs (ls + head)
        run: |
          echo "== outputs/docs ==" && ls -lah outputs/docs || true
          echo "== outputs/timelines ==" && ls -lah outputs/timelines || true
          echo "== reports/daily ==" && ls -lah reports/daily || true
          echo "== docs/digests ==" && ls -lah docs/digests || true
          echo "== docs/data ==" && ls -lah docs/data || true
          echo "== docs/site ==" && ls -lah docs/site || true
          echo "== docs root json ==" && ls -lah docs/*.json || true
          for f in outputs/docs/*.ndjson; do echo "---- $f"; head -n 2 "$f" || true; done
          for f in outputs/timelines/*.json; do echo "---- $f"; head -n 40 "$f" || true; done

      # 9) Commit results back to main (so the website can fetch fresh docs)
      - name: Commit results back to main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add outputs/docs outputs/timelines reports/daily docs/digests docs/data docs/site docs/*.json || true
          git commit -m "daily pipeline v2 $(date -u +'%F %T') [auto]" || echo "No changes to commit"
          git push || true

      # 10) Revalidate website (Next.js ISR / Vercel) — optional but recommended
      - name: Revalidate website
        if: ${{ env.REVALIDATE_URL != '' }}
        run: |
          echo "POST to $REVALIDATE_URL"
          curl -sS -X POST "$REVALIDATE_URL" \
            -H "Content-Type: application/json" \
            -d '{"paths":["/","/live"]}'
